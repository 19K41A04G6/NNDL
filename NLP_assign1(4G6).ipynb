{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_assign1(4G6)",
      "provenance": [],
      "authorship_tag": "ABX9TyN4giMcPWM6gIuauvQJBaYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19K41A04G6/NNDL/blob/main/NLP_assign1(4G6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Find BoW for the given paragraph? And also find stem and lemma words?"
      ],
      "metadata": {
        "id": "JIFG3O2QS3TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([paragraph])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['WORD','FREQUENCY']\n",
        "print(count_occur_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAMkWVF-Ut3e",
        "outputId": "1907278e-4034-4303-d65a-77a17b08529d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             WORD  FREQUENCY\n",
            "0         already          1\n",
            "1             and          1\n",
            "2    applications          1\n",
            "3             are          1\n",
            "4        articles          1\n",
            "5           books          1\n",
            "6           bound          1\n",
            "7          decide          1\n",
            "8         digital          1\n",
            "9       documents          1\n",
            "10         entire          1\n",
            "11           ever          1\n",
            "12             go          1\n",
            "13        growing          2\n",
            "14            has          1\n",
            "15           have          1\n",
            "16           here          1\n",
            "17           huge          1\n",
            "18         impact          1\n",
            "19             is          3\n",
            "20       language          1\n",
            "21          lives          1\n",
            "22          media          1\n",
            "23        natural          1\n",
            "24            nlp          1\n",
            "25            not          1\n",
            "26             of          2\n",
            "27             on          1\n",
            "28            one          1\n",
            "29             or          1\n",
            "30            our          1\n",
            "31     processing          1\n",
            "32     publishing          1\n",
            "33  summarization          1\n",
            "34     technology          1\n",
            "35           text          1\n",
            "36     thankfully          1\n",
            "37            the          1\n",
            "38           they          1\n",
            "39           this          1\n",
            "40          those          1\n",
            "41        through          1\n",
            "42           time          1\n",
            "43             to          3\n",
            "44         useful          1\n",
            "45        whether          1\n",
            "46          which          1\n",
            "47            who          1\n",
            "48           with          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = word_tokenize(paragraph)\n",
        "for word in text:\n",
        "  stemword = stemmer.stem(word)\n",
        "  print(stemword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "lrSUv9iwV56_",
        "outputId": "52b9362e-e891-4005-fba6-950be7ee008c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-de2bbf25bd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mstemword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stemmer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tSplit the above paragraph into sentences"
      ],
      "metadata": {
        "id": "19eHCx9KR5mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLA4KX2FQfqu",
        "outputId": "aaf7bf40-5cfc-4de2-bd58-655150e675a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but aren’t sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.Solving an NLP problem is a multi-stage process. We need to clean the unstructured text data first before we can even think about getting to the modelling stage. Cleaning the data consists of a few key steps.\"\n",
        "sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6v-RkepRSFM",
        "outputId": "79ae078b-a85a-4ae0-cc4b-dfa42719b06f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are you fascinated by the amount of text data available on the internet?',\n",
              " 'Are you looking for ways to work with this text data but aren’t sure where to begin?',\n",
              " 'Machines, after all, recognize numbers, not the letters of our language.',\n",
              " 'And that can be a tricky landscape to navigate in machine learning.Solving an NLP problem is a multi-stage process.',\n",
              " 'We need to clean the unstructured text data first before we can even think about getting to the modelling stage.',\n",
              " 'Cleaning the data consists of a few key steps.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tSplit the above paragraph into words"
      ],
      "metadata": {
        "id": "HNhw4oeBRqo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcOkpNtgSE0u",
        "outputId": "17c89b4e-b2c3-43dd-8c59-fc6262506d8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are',\n",
              " 'you',\n",
              " 'fascinated',\n",
              " 'by',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'text',\n",
              " 'data',\n",
              " 'available',\n",
              " 'on',\n",
              " 'the',\n",
              " 'internet',\n",
              " '?',\n",
              " 'Are',\n",
              " 'you',\n",
              " 'looking',\n",
              " 'for',\n",
              " 'ways',\n",
              " 'to',\n",
              " 'work',\n",
              " 'with',\n",
              " 'this',\n",
              " 'text',\n",
              " 'data',\n",
              " 'but',\n",
              " 'aren',\n",
              " '’',\n",
              " 't',\n",
              " 'sure',\n",
              " 'where',\n",
              " 'to',\n",
              " 'begin',\n",
              " '?',\n",
              " 'Machines',\n",
              " ',',\n",
              " 'after',\n",
              " 'all',\n",
              " ',',\n",
              " 'recognize',\n",
              " 'numbers',\n",
              " ',',\n",
              " 'not',\n",
              " 'the',\n",
              " 'letters',\n",
              " 'of',\n",
              " 'our',\n",
              " 'language',\n",
              " '.',\n",
              " 'And',\n",
              " 'that',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'tricky',\n",
              " 'landscape',\n",
              " 'to',\n",
              " 'navigate',\n",
              " 'in',\n",
              " 'machine',\n",
              " 'learning.Solving',\n",
              " 'an',\n",
              " 'NLP',\n",
              " 'problem',\n",
              " 'is',\n",
              " 'a',\n",
              " 'multi-stage',\n",
              " 'process',\n",
              " '.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'clean',\n",
              " 'the',\n",
              " 'unstructured',\n",
              " 'text',\n",
              " 'data',\n",
              " 'first',\n",
              " 'before',\n",
              " 'we',\n",
              " 'can',\n",
              " 'even',\n",
              " 'think',\n",
              " 'about',\n",
              " 'getting',\n",
              " 'to',\n",
              " 'the',\n",
              " 'modelling',\n",
              " 'stage',\n",
              " '.',\n",
              " 'Cleaning',\n",
              " 'the',\n",
              " 'data',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'a',\n",
              " 'few',\n",
              " 'key',\n",
              " 'steps',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}